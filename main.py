import os
import telebot
import re
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Qdrant

# ===== CONFIGURATION =====
# Get tokens from environment variables (safer for deployment)
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

if not TELEGRAM_BOT_TOKEN or not GEMINI_API_KEY:
    print("тЭМ Please set TELEGRAM_BOT_TOKEN and GEMINI_API_KEY environment variables")
    exit(1)

# ===== INITIALIZE COMPONENTS =====
bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# Global variables for RAG components
embedding_model = None
qdrant = None

# ===== RAG HELPER FUNCTIONS =====
def extract_text_from_markdown(md_path: str) -> str:
    """Extract text from a Markdown (.md) file."""
    all_text = ""
    try:
        with open(md_path, 'r', encoding='utf-8') as file:
            all_text = file.read()
            print(f"ЁЯУД Successfully loaded content from {md_path}")
    except Exception as e:
        print(f"тЭМ Error reading file: {e}")
        all_text = f"Error: {e}"
    return all_text

def clean_text(text: str) -> str:
    """Clean Markdown or OCR text."""
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\|\s*-+\s*\|(\s*-+\s*\|)*', '', text)
    text = re.sub(r'^\s*(\|\s*){2,}\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'!', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text

def chunk_text_from_string(text: str):
    """Split text into chunks for processing."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=100,
        separators=["\n\n", "\n", "ред", ".", " ", ""]
    )
    documents = [{"content": text, "source": "hindi_teacher_guide"}]
    chunks = []
    for doc in documents:
        split_docs = text_splitter.create_documents(
            [doc["content"]],
            metadatas=[{"source": doc["source"]}]
        )
        chunks.extend(split_docs)
    
    chunks = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 50]
    return chunks

def initialize_rag_system():
    """Initialize the RAG system with the teacher training content."""
    global embedding_model, qdrant
    
    try:
        # Try different possible file paths
        possible_paths = [
            "ocr_result.md"
        ]
        
        md_path = None
        for path in possible_paths:
            if os.path.exists(path):
                md_path = path
                break
        
        if not md_path:
            print("тЭМ ocr_result.md file not found in any expected location")
            return False
        
        print(f"ЁЯФД Loading content from: {md_path}")
        markdown_text = extract_text_from_markdown(md_path)
        
        if not markdown_text or markdown_text.startswith("Error:"):
            print("тЭМ Failed to load markdown file")
            return False
            
        cleaned_text = clean_text(markdown_text)
        print(f"ЁЯУД Cleaned text length: {len(cleaned_text)} characters")
        
        chunks = chunk_text_from_string(cleaned_text)
        print(f"ЁЯУК Created {len(chunks)} text chunks")
        
        if len(chunks) == 0:
            print("тЭМ No chunks created - check your content")
            return False
        
        # Initialize embedding model
        print("ЁЯФД Initializing embedding model...")
        embedding_model = HuggingFaceEmbeddings(model_name="AkshitaS/bhasha-embed-v0")
        print("тЬЕ Embedding model loaded")
        
        # Create vector database
        print("ЁЯФД Creating vector database...")
        qdrant = Qdrant.from_documents(
            chunks,
            embedding_model,
            location=":memory:",
            collection_name="teacher_training_documents",
        )
        print("тЬЕ Vector database created")
        
        # Test the system
        test_query = "рдЧрдгрд┐рдд рдкрд╛рда рдпреЛрдЬрдирд╛"
        test_embedding = embedding_model.embed_query(test_query)
        test_results = qdrant.similarity_search_by_vector(test_embedding, k=3)
        
        if test_results:
            print(f"тЬЕ RAG system working! Found {len(test_results)} test results")
            return True
        else:
            print("тЪая╕П Vector database created but no test results found")
            return False
        
    except Exception as e:
        print(f"тЭМ Error initializing RAG system: {e}")
        import traceback
        traceback.print_exc()
        return False

def query_rag_system(query: str):
    """Query the RAG system and get response."""
    global embedding_model, qdrant
    
    if not embedding_model or not qdrant:
        return "тЭМ RAG system not initialized. Please contact administrator."
    
    try:
        print(f"ЁЯФН Processing query: {query}")
        query_embedding = embedding_model.embed_query(query)
        top_matches = qdrant.similarity_search_by_vector(query_embedding, k=7)
        print(f"ЁЯУК Found {len(top_matches)} matching contexts")
        
        if not top_matches:
            return "рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореБрдЭреЗ рдЖрдкрдХреЗ рдкреНрд░рд╢реНрди рд╕реЗ рд╕рдВрдмрдВрдзрд┐рдд рдЬрд╛рдирдХрд╛рд░реА рдирд╣реАрдВ рдорд┐рд▓реАред рдХреГрдкрдпрд╛ рдЕрдкрдирд╛ рдкреНрд░рд╢реНрди рдЕрд▓рдЧ рддрд░реАрдХреЗ рд╕реЗ рдкреВрдЫреЗрдВред"
        
        response = pass_to_llm_for_teacher_training(query, top_matches)
        return response
        
    except Exception as e:
        print(f"тЭМ Error in RAG query: {e}")
        return f"рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдПрдХ рддрдХрдиреАрдХреА рд╕рдорд╕реНрдпрд╛ рдЖрдИ рд╣реИред рдХреГрдкрдпрд╛ рдмрд╛рдж рдореЗрдВ рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред"

def pass_to_llm_for_teacher_training(query, top_matches):
    """Use Gemini to generate response based on retrieved context."""
    system_prompt = """
рдЖрдк рдПрдХ рд╕рд╣рд╛рдпрдХ рд╣реИрдВ рдЬреЛ рд╢рд┐рдХреНрд╖рдХреЛрдВ рдХреА рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдЧрд╛рдЗрдб рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рд╣рд┐рдВрджреА рдореЗрдВ рдЙрддреНрддрд░ рджреЗрддрд╛ рд╣реИред

рдЖрдкрдХрд╛ рдЙрддреНрддрд░ рдЗрди рд╕рд┐рджреНрдзрд╛рдВрддреЛрдВ рдкрд░ рдЖрдзрд╛рд░рд┐рдд рд╣реЛрдирд╛ рдЪрд╛рд╣рд┐рдП:
1. рдЕрд╕реНрдкрд╖реНрдЯ рдпрд╛ рдЕрдзреВрд░реА рдкреВрдЫреА рдЧрдИ рдмрд╛рддреЛрдВ рдХреЛ рд╕рдордЭрдХрд░ рд╕реНрдкрд╖реНрдЯ рд╕реБрдЭрд╛рд╡ рджреЗрдВред
2. рд╕рдкреНрддрд╛рд╣, рджрд┐рди, рдФрд░ рд╡рд┐рд╖рдп рдХреЗ рдЖрдзрд╛рд░ рдкрд░ structured рддрд░реАрдХреЗ рд╕реЗ рдЬрд╛рдирдХрд╛рд░реА рджреЗрдВред
3. рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рджреНрд╡рд╛рд░рд╛ рдорд╛рдБрдЧрд╛ рдЧрдпрд╛ рдЖрдЙрдЯрдкреБрдЯ рджреЗрдВ тАФ рдЬреИрд╕реЗ рдЪрд┐рдЯ-рд╢реАрдЯ, remedial рд╕рдВрд╕реНрдХрд░рдг, рдпрд╛ рдХрд┐рд╕реА рдЧрддрд┐рд╡рд┐рдзрд┐ рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдгред
4. рд╣рдореЗрд╢рд╛ рджрд┐рдП рдЧрдП рд╕рдВрджрд░реНрдн (context) рд╕реЗ рд╣реА рдЬрд╛рдирдХрд╛рд░реА рджреЗрдВред рдЕрдЧрд░ рдЬрд╛рдирдХрд╛рд░реА рдореМрдЬреВрдж рдирд╣реАрдВ рд╣реИ, рддреЛ рд╕рд╛рдлрд╝-рд╕рд╛рдлрд╝ рдХрд╣реЗрдВ рдХрд┐ рд╡рд╣ рдЧрд╛рдЗрдб рдореЗрдВ рдирд╣реАрдВ рд╣реИред
5. рдЙрддреНрддрд░ рдХреЗрд╡рд▓ рд╣рд┐рдВрджреА рдореЗрдВ рджреЗрдВ рдФрд░ рд╕реНрд░реЛрдд рдХрд╛ рд╕рд╣реА-рд╕рд╣реА рд╕рдВрджрд░реНрдн рджреЗрдВ (рдЙрджрд╛рд╣рд░рдг: рд╕рдкреНрддрд╛рд╣ 3, рджрд┐рди 2)ред
6. рдЙрддреНрддрд░ рдХреЗ рдЕрдВрдд рдореЗрдВ рд╣рдореЗрд╢рд╛ "рдзрдиреНрдпрд╡рд╛рдж! рдпрджрд┐ рдЖрдкрдХрд╛ рдХреЛрдИ рдФрд░ рдкреНрд░рд╢реНрди рд╣реИ рддреЛ рдмреЗрдЭрд┐рдЭрдХ рдкреВрдЫреЗрдВред" рдЬреЛрдбрд╝реЗрдВред
"""

    context_text = "\n\n---\n\n".join(match.page_content for match in top_matches)
    full_prompt = f"""{system_prompt}

рд╕рдВрджрд░реНрдн:
{context_text}

рдкреНрд░рд╢реНрди:
{query}

рдЙрддреНрддрд░:"""

    try:
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        print(f"тЭМ Gemini API Error: {e}")
        return "рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореИрдВ рдЗрд╕ рд╕рдордп рдЖрдкрдХреА рд╕рд╣рд╛рдпрддрд╛ рдирд╣реАрдВ рдХрд░ рдкрд╛ рд░рд╣рд╛ред рдХреГрдкрдпрд╛ рдмрд╛рдж рдореЗрдВ рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред"

# ===== TELEGRAM BOT HANDLERS =====
@bot.message_handler(commands=['start', 'help'])
def send_welcome(message):
    """Handle start and help commands."""
    user_name = message.from_user.first_name or "рд╢рд┐рдХреНрд╖рдХ рдЬреА"
    
    welcome_message = f"""
рдирдорд╕реНрддреЗ {user_name}! ЁЯЩП

рдореИрдВ Central Square Foundation Bot рд╣реВрдБред рдЖрдЬ рдореИрдВ рдЖрдкрдХреА рддреАрд╕рд░реА рдХрдХреНрд╖рд╛ рдХреЗ рдЧрдгрд┐рдд рд╢рд┐рдХреНрд╖рдг рдореЗрдВ рдХреИрд╕реЗ рд╕рд╣рд╛рдпрддрд╛ рдХрд░ рд╕рдХрддрд╛ рд╣реВрдБ? 

рдореИрдВ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдореЗрдВ рдЖрдкрдХреА рдорджрдж рдХрд░ рд╕рдХрддрд╛ рд╣реВрдБ:
ЁЯУЪ рдЧрдгрд┐рдд рдкрд╛рда рдпреЛрдЬрдирд╛рдУрдВ рдХреЛ рд╕рдордЭрдиреЗ рдореЗрдВ
ЁЯУЦ рд╢рд┐рдХреНрд╖рдХ рдЧрд╛рдЗрдб рдореЗрдВ рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╕рд╛рдордЧреНрд░реА рдЦреЛрдЬрдиреЗ рдореЗрдВ  
ЁЯОп рдЧрдгрд┐рдд рдЕрд╡рдзрд╛рд░рдгрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рд╢рд┐рдХреНрд╖рдг рд░рдгрдиреАрддрд┐рдпреЛрдВ рдХреЛ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдореЗрдВ
ЁЯТб рдкрд╛рда рдпреЛрдЬрдирд╛ рдФрд░ рдЧрддрд┐рд╡рд┐рдзрд┐рдпреЛрдВ рдХреЗ рд╕реБрдЭрд╛рд╡ рджреЗрдиреЗ рдореЗрдВ

рдХреГрдкрдпрд╛ рдЕрдкрдирд╛ рдкреНрд░рд╢реНрди рд╣рд┐рдВрджреА рдореЗрдВ рдкреВрдЫреЗрдВред рдореИрдВ рдЖрдкрдХреА рд╕рд╣рд╛рдпрддрд╛ рдХреЗ рд▓рд┐рдП рдпрд╣рд╛рдБ рд╣реВрдБ! ЁЯШК
"""
    
    bot.reply_to(message, welcome_message)

@bot.message_handler(func=lambda message: True)
def handle_teacher_query(message):
    """Handle all text messages from teachers."""
    user_name = message.from_user.first_name or "рд╢рд┐рдХреНрд╖рдХ рдЬреА"
    user_query = message.text.strip()
    
    bot.send_chat_action(message.chat.id, 'typing')
    
    try:
        response = query_rag_system(user_query)
        bot.reply_to(message, response)
        
    except Exception as e:
        error_message = f"рдХреНрд╖рдорд╛ рдХрд░реЗрдВ {user_name}, рдореБрдЭреЗ рдПрдХ рддрдХрдиреАрдХреА рд╕рдорд╕реНрдпрд╛ рдЖрдИ рд╣реИред рдХреГрдкрдпрд╛ рдмрд╛рдж рдореЗрдВ рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред"
        bot.reply_to(message, error_message)
        print(f"тЭМ Error handling message: {e}")

# ===== MAIN EXECUTION =====
def main():
    """Main function to start the bot."""
    print("ЁЯЪА Starting Central Square Foundation Teacher Training Bot...")
    
    if not initialize_rag_system():
        print("тЭМ Failed to initialize RAG system. Bot cannot start.")
        return
    
    print("тЬЕ Bot is ready to serve teachers!")
    print("ЁЯФД Starting bot polling...")
    
    try:
        bot.infinity_polling(timeout=10, long_polling_timeout=5)
    except Exception as e:
        print(f"тЭМ Bot polling error: {e}")

if __name__ == "__main__":
    main()